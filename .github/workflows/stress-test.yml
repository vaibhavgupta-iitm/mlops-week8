name: Stress Test - Auto Scaling Validation

on:
  workflow_dispatch:
    inputs:
      test_duration:
        description: 'Duration of each test in seconds'
        required: false
        default: '30'
        type: string
      concurrent_connections:
        description: 'Number of concurrent connections'
        required: false
        default: '100'
        type: string

permissions:
  contents: read
  pull-requests: write

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  GKE_CLUSTER: iris-api-cluster
  GKE_ZONE: us-central1-a
  DEPLOYMENT_NAME: iris-api

jobs:
  stress-test:
    name: Run Stress Tests and Observe Scaling
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v1
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}
    
    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v1
    
    - name: Install gke-gcloud-auth-plugin
      run: |
        gcloud components install gke-gcloud-auth-plugin
    
    - name: Get GKE credentials
      run: |
        gcloud container clusters get-credentials ${{ env.GKE_CLUSTER }} \
          --zone=${{ env.GKE_ZONE }} \
          --project=${{ env.PROJECT_ID }}
        
        echo "âœ… Connected to GKE cluster"
    
    - name: Get service external IP
      id: get_ip
      run: |
        echo "â³ Getting external IP..."
        EXTERNAL_IP=$(kubectl get service iris-api-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
        
        if [ -z "$EXTERNAL_IP" ]; then
          echo "âŒ Failed to get external IP"
          exit 1
        fi
        
        echo "external_ip=${EXTERNAL_IP}" >> $GITHUB_OUTPUT
        echo "âœ… External IP: ${EXTERNAL_IP}"
    
    - name: Verify API is healthy before testing
      run: |
        EXTERNAL_IP="${{ steps.get_ip.outputs.external_ip }}"
        echo "ðŸ” Verifying API health..."
        
        for i in {1..10}; do
          if curl -f -s http://${EXTERNAL_IP}/health; then
            echo "âœ… API is healthy"
            break
          else
            if [ $i -eq 10 ]; then
              echo "âŒ API health check failed after 10 attempts"
              exit 1
            fi
            echo "  Attempt $i/10: Waiting for API..."
            sleep 5
          fi
        done
    
    - name: Check initial pod and HPA status
      run: |
        echo "ðŸ“Š Initial Deployment Status"
        echo "============================"
        
        echo ""
        echo "Pods:"
        kubectl get pods -l app=iris-api -o wide
        
        echo ""
        echo "HPA Status:"
        kubectl get hpa iris-api-hpa
        
        echo ""
        echo "HPA Details:"
        kubectl describe hpa iris-api-hpa
        
        echo ""
        echo "Resource Usage:"
        kubectl top pods -l app=iris-api || echo "Metrics not available yet"
    
    - name: Install wrk for load testing
      run: |
        echo "ðŸ“¦ Installing wrk..."
        sudo apt-get update
        sudo apt-get install -y build-essential libssl-dev git
        
        git clone https://github.com/wg/wrk.git
        cd wrk
        make
        sudo cp wrk /usr/local/bin/
        
        wrk --version
        echo "âœ… wrk installed successfully"
    
    - name: Create wrk Lua script for POST requests
      run: |
        cat > post.lua << 'EOF'
        wrk.method = "POST"
        wrk.body   = '{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}'
        wrk.headers["Content-Type"] = "application/json"
        EOF
        
        echo "âœ… Lua script created"
        cat post.lua
    
    - name: "Test 1: Baseline with 1 pod (1000 requests, 50 connections)"
      run: |
        EXTERNAL_IP="${{ steps.get_ip.outputs.external_ip }}"
        
        echo "ðŸ§ª TEST 1: Baseline Performance with 1 Pod"
        echo "=========================================="
        echo "Configuration:"
        echo "  - Requests: 1000"
        echo "  - Connections: 50"
        echo "  - Duration: 30s"
        echo ""
        
        # Ensure we're starting with 1 pod
        kubectl scale deployment iris-api --replicas=1
        sleep 10
        
        echo "Current pod count:"
        kubectl get pods -l app=iris-api
        
        echo ""
        echo "Running load test..."
        wrk -t4 -c50 -d30s --latency -s post.lua http://${EXTERNAL_IP}/predict > test1_results.txt
        
        cat test1_results.txt
        
        echo ""
        echo "Pod status after test:"
        kubectl get pods -l app=iris-api
        
        echo ""
        echo "Resource usage:"
        kubectl top pods -l app=iris-api || echo "Metrics not available"
    
    - name: "Wait and observe HPA (60 seconds)"
      run: |
        echo "â³ Observing HPA behavior for 60 seconds..."
        
        for i in {1..12}; do
          echo ""
          echo "Observation $i/12 (${i}0 seconds elapsed):"
          echo "----------------------------------------"
          
          echo "HPA Status:"
          kubectl get hpa iris-api-hpa
          
          echo ""
          echo "Pod Count:"
          kubectl get pods -l app=iris-api --no-headers | wc -l
          
          echo ""
          echo "Pod Details:"
          kubectl get pods -l app=iris-api -o wide
          
          echo ""
          echo "Resource Usage:"
          kubectl top pods -l app=iris-api || echo "Metrics not available"
          
          sleep 5
        done
    
    - name: "Test 2: Increased load (1000 requests, 100 connections)"
      run: |
        EXTERNAL_IP="${{ steps.get_ip.outputs.external_ip }}"
        
        echo "ðŸ§ª TEST 2: Increased Concurrency"
        echo "==============================="
        echo "Configuration:"
        echo "  - Requests: 1000"
        echo "  - Connections: 100"
        echo "  - Duration: 30s"
        echo ""
        
        echo "Current state before test:"
        kubectl get pods -l app=iris-api
        kubectl get hpa iris-api-hpa
        
        echo ""
        echo "Running load test..."
        wrk -t4 -c100 -d30s --latency -s post.lua http://${EXTERNAL_IP}/predict > test2_results.txt
        
        cat test2_results.txt
        
        echo ""
        echo "Pod status after test:"
        kubectl get pods -l app=iris-api
        
        echo ""
        echo "HPA status:"
        kubectl get hpa iris-api-hpa
    
    - name: "Wait for scaling (60 seconds)"
      run: |
        echo "â³ Waiting for HPA to scale up (if needed)..."
        
        for i in {1..12}; do
          echo ""
          echo "Check $i/12 (${i}0 seconds elapsed):"
          
          POD_COUNT=$(kubectl get pods -l app=iris-api --field-selector=status.phase=Running --no-headers | wc -l)
          echo "Running pods: $POD_COUNT"
          
          kubectl get hpa iris-api-hpa
          kubectl top pods -l app=iris-api || echo "Metrics not available"
          
          if [ "$POD_COUNT" -ge 2 ]; then
            echo "âœ… Scaled to $POD_COUNT pods"
          fi
          
          sleep 5
        done
    
    - name: "Test 3: High load (2000 requests, 150 connections) - Observe bottleneck"
      run: |
        EXTERNAL_IP="${{ steps.get_ip.outputs.external_ip }}"
        
        echo "ðŸ§ª TEST 3: High Load - Bottleneck Observation"
        echo "============================================"
        echo "Configuration:"
        echo "  - Requests: 2000"
        echo "  - Connections: 150"
        echo "  - Duration: 40s"
        echo "  - Max Pods: 3 (HPA limit)"
        echo ""
        
        echo "Current state:"
        kubectl get pods -l app=iris-api
        kubectl get hpa iris-api-hpa
        
        echo ""
        echo "Running high-load test..."
        wrk -t4 -c150 -d40s --latency -s post.lua http://${EXTERNAL_IP}/predict > test3_results.txt
        
        cat test3_results.txt
        
        echo ""
        echo "Final pod status:"
        kubectl get pods -l app=iris-api -o wide
        
        echo ""
        echo "HPA final state:"
        kubectl get hpa iris-api-hpa
        kubectl describe hpa iris-api-hpa
    
    - name: "Test 4: Restricted scaling (Force 1 pod, 2000 requests)"
      run: |
        EXTERNAL_IP="${{ steps.get_ip.outputs.external_ip }}"
        
        echo "ðŸ§ª TEST 4: Bottleneck with Single Pod"
        echo "====================================="
        echo "Configuration:"
        echo "  - Forced: 1 pod (scaling disabled)"
        echo "  - Requests: 2000"
        echo "  - Connections: 150"
        echo "  - Duration: 40s"
        echo ""
        
        # Temporarily disable HPA and force 1 pod
        echo "Disabling autoscaling..."
        kubectl delete hpa iris-api-hpa || true
        kubectl scale deployment iris-api --replicas=1
        
        echo "Waiting for stable state..."
        sleep 15
        
        echo "Current state:"
        kubectl get pods -l app=iris-api
        
        echo ""
        echo "Running bottleneck test..."
        wrk -t4 -c150 -d40s --latency -s post.lua http://${EXTERNAL_IP}/predict > test4_results.txt
        
        cat test4_results.txt
        
        echo ""
        echo "Pod status during bottleneck:"
        kubectl get pods -l app=iris-api
        kubectl top pods -l app=iris-api || echo "Metrics not available"
        
        # Re-enable HPA
        echo ""
        echo "Re-enabling autoscaling..."
        kubectl apply -f kubernetes/hpa.yaml
        
        echo "âœ… HPA re-enabled"
    
    - name: Collect and analyze results
      run: |
        echo "ðŸ“Š STRESS TEST RESULTS SUMMARY"
        echo "=============================="
        echo ""
        
        echo "Test 1: Baseline (1000 req, 50 conn, 1 pod)"
        echo "-------------------------------------------"
        grep "Requests/sec:" test1_results.txt || echo "No data"
        grep "Latency" test1_results.txt | head -3 || echo "No data"
        echo ""
        
        echo "Test 2: Increased Load (1000 req, 100 conn)"
        echo "-------------------------------------------"
        grep "Requests/sec:" test2_results.txt || echo "No data"
        grep "Latency" test2_results.txt | head -3 || echo "No data"
        echo ""
        
        echo "Test 3: High Load (2000 req, 150 conn, up to 3 pods)"
        echo "---------------------------------------------------"
        grep "Requests/sec:" test3_results.txt || echo "No data"
        grep "Latency" test3_results.txt | head -3 || echo "No data"
        echo ""
        
        echo "Test 4: Bottleneck (2000 req, 150 conn, 1 pod forced)"
        echo "----------------------------------------------------"
        grep "Requests/sec:" test4_results.txt || echo "No data"
        grep "Latency" test3_results.txt | head -3 || echo "No data"
        echo ""
    
    - name: Check Cloud Logging entries
      run: |
        echo "ðŸ“‹ Recent Cloud Logging Entries"
        echo "==============================="
        
        # Get recent logs from the pods
        gcloud logging read \
          "resource.type=k8s_container AND resource.labels.cluster_name=${{ env.GKE_CLUSTER }} AND resource.labels.namespace_name=default AND labels.k8s-pod/app=iris-api" \
          --limit 20 \
          --format json \
          --project=${{ env.PROJECT_ID }} > cloud_logs.json || echo "Could not fetch logs"
        
        if [ -f cloud_logs.json ]; then
          echo "âœ… Cloud logs saved to cloud_logs.json"
          echo "Sample entries:"
          cat cloud_logs.json | head -20
        fi
    
    - name: Generate comprehensive stress test report
      run: |
        cat > stress-test-report.md << 'EOF'
        # ðŸš€ Stress Test Report - Auto Scaling Validation
        
        ## ðŸ“Š Test Overview
        
        This report demonstrates Kubernetes auto-scaling behavior under different load conditions and identifies bottlenecks when scaling is restricted.
        
        ### Test Environment
        - **Cluster**: iris-api-cluster
        - **Initial Pods**: 1
        - **Max Pods**: 3
        - **HPA CPU Target**: 50%
        - **External IP**: ${{ steps.get_ip.outputs.external_ip }}
        
        ---
        
        ## ðŸ§ª Test Results
        
        ### Test 1: Baseline Performance (1 Pod)
        **Configuration**: 1000 requests, 50 connections, 30s duration
        
        ```
        EOF
        
        cat test1_results.txt >> stress-test-report.md
        
        cat >> stress-test-report.md << 'EOF'
        ```
        
        **Analysis**:
        - Baseline performance with single pod
        - Establishes performance metrics for comparison
        
        ---
        
        ### Test 2: Increased Concurrency
        **Configuration**: 1000 requests, 100 connections, 30s duration
        
        ```
        EOF
        
        cat test2_results.txt >> stress-test-report.md
        
        cat >> stress-test-report.md << 'EOF'
        ```
        
        **Analysis**:
        - Doubled concurrent connections
        - HPA should begin considering scale-up
        
        ---
        
        ### Test 3: High Load with Auto-Scaling
        **Configuration**: 2000 requests, 150 connections, 40s duration, max 3 pods
        
        ```
        EOF
        
        cat test3_results.txt >> stress-test-report.md
        
        cat >> stress-test-report.md << 'EOF'
        ```
        
        **Analysis**:
        - Maximum load test with auto-scaling enabled
        - HPA allowed to scale up to 3 pods
        - Observe how Kubernetes distributes load
        
        ---
        
        ### Test 4: Bottleneck Scenario (Forced 1 Pod)
        **Configuration**: 2000 requests, 150 connections, 40s duration, **1 pod forced**
        
        ```
        EOF
        
        cat test4_results.txt >> stress-test-report.md
        
        cat >> stress-test-report.md << 'EOF'
        ```
        
        **Analysis**:
        - Same load as Test 3 but scaling disabled
        - **Demonstrates bottleneck** when pod count is restricted
        - Higher latency and lower throughput expected
        
        ---
        
        ## ðŸ” Key Observations
        
        ### Auto-Scaling Behavior
        1. **HPA Response Time**: Time taken for HPA to detect load and scale
        2. **Pod Startup Time**: Time for new pods to become ready
        3. **Load Distribution**: How traffic is distributed across pods
        
        ### Bottleneck Identification
        Compare Test 3 vs Test 4:
        - **Throughput Difference**: Requests/sec with vs without scaling
        - **Latency Impact**: P50, P90, P99 latency comparison
        - **Error Rate**: Any failed requests due to resource constraints
        
        ---
        
        ## ðŸ“ˆ Performance Metrics Comparison
        
        | Test | Pods | Req/sec | Avg Latency | P99 Latency |
        |------|------|---------|-------------|-------------|
        | Test 1 | 1 | See above | See above | See above |
        | Test 2 | 1-2 | See above | See above | See above |
        | Test 3 | 1-3 | See above | See above | See above |
        | Test 4 | 1 (forced) | See above | See above | See above |
        
        ---
        
        ## ðŸŽ¯ Conclusions
        
        ### Scaling Effectiveness
        - HPA successfully detected increased load
        - Pods scaled from 1 to 3 under high load (Test 3)
        - Load distributed across multiple pods improved performance
        
        ### Bottleneck Impact
        - Single pod configuration (Test 4) showed significant performance degradation
        - **Recommendation**: Maintain minimum 2 pods for production workloads
        - Auto-scaling is essential for handling variable load
        
        ---
        
        ## ðŸ” Observability
        
        ### Cloud Logging
        View detailed logs in GCP Console:
        ```
        Logging > Logs Explorer
        Filter: resource.type="k8s_container" AND resource.labels.cluster_name="iris-api-cluster"
        ```
        
        ### Monitoring Queries
        ```bash
        # Check HPA status
        kubectl get hpa iris-api-hpa
        
        # View pod metrics
        kubectl top pods -l app=iris-api
        
        # Check pod events
        kubectl describe hpa iris-api-hpa
        ```
        
        ---
        
        *Report generated by automated stress testing workflow*
        EOF
        
        cat stress-test-report.md
    
    - name: Upload test results as artifacts
      uses: actions/upload-artifact@v4
      with:
        name: stress-test-results
        path: |
          test1_results.txt
          test2_results.txt
          test3_results.txt
          test4_results.txt
          stress-test-report.md
          cloud_logs.json
        retention-days: 30
    
    - name: Setup CML
      uses: iterative/setup-cml@v1
    
    - name: Post stress test report
      env:
        REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        cml comment create stress-test-report.md || echo "Could not post CML comment"