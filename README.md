# ğŸ›¡ï¸ Data Poisoning Detection & Mitigation Framework

## Overview

This framework extends the IRIS Classification MLOps pipeline with comprehensive **data poisoning analysis** capabilities. It demonstrates how malicious data corruption affects ML models and provides automated detection and mitigation strategies.

### ğŸ¯ What This Framework Does

- **Simulates 4 types of poisoning attacks** on training data
- **Validates data quality** using statistical and ML-based methods
- **Demonstrates mitigation strategies** with effectiveness metrics
- **Calculates data quantity requirements** when quality is compromised
- **Tracks all experiments in MLflow** with comprehensive reporting
- **Generates visualizations** comparing attack impacts

---

## ğŸ“‹ Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Attack Types](#attack-types)
- [Validation Methods](#validation-methods)
- [Mitigation Strategies](#mitigation-strategies)
- [Understanding Results](#understanding-results)
- [Data Quantity vs Quality](#data-quantity-vs-quality)
- [MLflow Integration](#mlflow-integration)
- [CI/CD Integration](#cicd-integration)
- [API Reference](#api-reference)
- [Advanced Usage](#advanced-usage)
- [Troubleshooting](#troubleshooting)

---

## âœ¨ Features

### ğŸ­ Attack Simulation

| Attack Type | Description | Stealthiness | Impact |
|-------------|-------------|--------------|--------|
| **Random Noise** | Add Gaussian noise to features | Low | Moderate |
| **Label Flip** | Randomly change correct labels | High | Severe |
| **Targeted** | Manipulate specific class | Very High | Asymmetric |
| **Backdoor** | Insert trigger pattern | Extreme | Complete Control |

### ğŸ” Detection Capabilities

- **Statistical Outlier Detection**: Z-score based anomaly identification
- **Isolation Forest**: ML-based anomaly detection
- **Label Distribution Analysis**: Chi-square test for imbalance
- **Feature Range Validation**: Domain-specific boundary checks
- **Data Hash Verification**: Cryptographic integrity checking
- **Comprehensive Health Score**: 0-100 quality metric

### ğŸ› ï¸ Mitigation Techniques

- **Outlier Removal**: Z-score and Isolation Forest filtering
- **Feature Clipping**: Boundary enforcement
- **Ensemble Filtering**: Cross-validation based sample removal
- **Data Augmentation**: Strategic clean data collection
- **Hybrid Approaches**: Multi-method combination

### ğŸ“Š Analysis & Reporting

- **MLflow Integration**: Automatic experiment tracking
- **Visual Comparisons**: Accuracy degradation charts
- **Data Requirement Plots**: Quantity vs quality trade-offs
- **Markdown Reports**: Comprehensive analysis documents
- **GitHub Actions**: Automated validation pipeline

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Poisoning Framework                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DataPoisoner â”‚    â”‚DataValidator â”‚    â”‚DataMitigator â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Random     â”‚    â”‚ â€¢ Z-score    â”‚    â”‚ â€¢ Remove     â”‚
â”‚   Noise      â”‚    â”‚ â€¢ Isolation  â”‚    â”‚   Outliers   â”‚
â”‚ â€¢ Label Flip â”‚    â”‚   Forest     â”‚    â”‚ â€¢ Feature    â”‚
â”‚ â€¢ Targeted   â”‚    â”‚ â€¢ Chi-square â”‚    â”‚   Clipping   â”‚
â”‚ â€¢ Backdoor   â”‚    â”‚ â€¢ Range      â”‚    â”‚ â€¢ Ensemble   â”‚
â”‚              â”‚    â”‚   Checks     â”‚    â”‚   Filtering  â”‚
â”‚              â”‚    â”‚ â€¢ Hash       â”‚    â”‚ â€¢ Calculate  â”‚
â”‚              â”‚    â”‚   Verify     â”‚    â”‚   Data Needs â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  PoisoningExperiment     â”‚
                â”‚  (Orchestrator)          â”‚
                â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                â”‚ â€¢ Run baseline           â”‚
                â”‚ â€¢ Apply attacks          â”‚
                â”‚ â€¢ Validate data          â”‚
                â”‚ â€¢ Train models           â”‚
                â”‚ â€¢ Attempt mitigation     â”‚
                â”‚ â€¢ Generate reports       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚      MLflow Tracking     â”‚
                â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                â”‚ â€¢ 13 experiments         â”‚
                â”‚ â€¢ Metrics & parameters   â”‚
                â”‚ â€¢ Charts & reports       â”‚
                â”‚ â€¢ Model artifacts        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Installation

### Prerequisites

- Python 3.9+
- MLflow server (optional, can use local tracking)
- Google Cloud SDK (for CI/CD integration)

### Install Dependencies

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git
cd YOUR_REPO

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install base requirements
pip install -r requirements.txt

# Install additional dependencies for poisoning analysis
pip install scipy seaborn matplotlib
```

### Verify Installation

```bash
python -c "from data_poisoning import DataPoisoner, DataValidator, DataMitigator; print('âœ… Installation successful!')"
```

---

## âš¡ Quick Start

### Option 1: Run All Experiments

```bash
# Run comprehensive analysis (5%, 10%, 50% poison rates)
python run_poisoning_experiments.py \
  --data-path iris-dvc-pipeline/v1_data.csv \
  --mlflow-tracking-uri http://localhost:5001 \
  --poison-rates 0.05,0.10,0.50

# View results
mlflow ui --port 5001
# Open: http://localhost:5001
```

### Option 2: Quick Validation

```python
from data_poisoning import DataValidator
import pandas as pd

# Load your data
data = pd.read_csv('iris-dvc-pipeline/v1_data.csv')

# Validate
validator = DataValidator()
report = validator.comprehensive_validation(data)

print(f"Health Score: {report['overall_health_score']:.1f}/100")
print(f"Is Clean: {report['is_clean']}")
print(f"Outliers: {report['statistical_outliers']['total_outliers']}")
```

### Option 3: Test Single Attack

```python
from data_poisoning import DataPoisoner
import pandas as pd

# Load clean data
data = pd.read_csv('iris-dvc-pipeline/v1_data.csv')

# Create poisoner
poisoner = DataPoisoner(random_state=42)

# Apply 10% random noise
poisoned_data, indices = poisoner.poison_random_noise(data, poison_rate=0.10)

print(f"Poisoned {len(indices)} out of {len(data)} samples")
print(f"Poisoned indices: {indices}")
```

---

## ğŸ­ Attack Types

### 1. Random Noise Poisoning

**Mechanism**: Add extreme Gaussian noise (3Ïƒ) to all features

```python
poisoner = DataPoisoner()
poisoned_data, indices = poisoner.poison_random_noise(
    data, 
    poison_rate=0.10,  # 10% of samples
    noise_level=3.0     # 3 standard deviations
)
```

**Characteristics**:
- **Detection**: Easy (statistical outliers)
- **Impact**: Moderate (model partially resilient)
- **Mitigation**: 70-80% recovery rate

**Example Effect**:
```
Original: [5.1, 3.5, 1.4, 0.2]
Poisoned: [8.7, 1.2, 4.9, 2.8]  â† Extreme values
```

**Real-world Analogy**: Sensor malfunction, corrupted data transmission

---

### 2. Label Flip Poisoning

**Mechanism**: Randomly change labels while keeping features unchanged

```python
poisoner = DataPoisoner()
poisoned_data, indices = poisoner.poison_label_flip(
    data,
    poison_rate=0.10
)
```

**Characteristics**:
- **Detection**: Hard (features look normal)
- **Impact**: Severe (direct supervision corruption)
- **Mitigation**: 50-60% recovery rate

**Example Effect**:
```
Original: features=[5.1, 3.5, 1.4, 0.2], label="setosa"  âœ…
Poisoned: features=[5.1, 3.5, 1.4, 0.2], label="virginica"  âŒ
```

**Real-world Analogy**: Malicious annotators, human labeling errors

---

### 3. Targeted Attack

**Mechanism**: Subtly shift specific class features toward another class

```python
poisoner = DataPoisoner()
poisoned_data, indices = poisoner.poison_targeted_attack(
    data,
    poison_rate=0.10,
    target_class='setosa',
    misclassify_as='virginica'
)
```

**Characteristics**:
- **Detection**: Very hard (subtle changes)
- **Impact**: Asymmetric (specific class damaged)
- **Mitigation**: 55-65% recovery rate

**Example Effect**:
```
Original Setosa: [5.1, 3.5, 1.4, 0.2]
Virginica avg:   [6.5, 3.0, 5.5, 2.0]
Poisoned (70% shift): [6.1, 3.2, 4.3, 1.5]  â† Looks like virginica
```

**Real-world Analogy**: Adversarial data injection, competitor sabotage

---

### 4. Backdoor Attack

**Mechanism**: Insert trigger pattern that causes specific misclassification

```python
poisoner = DataPoisoner()
poisoned_data, indices = poisoner.poison_backdoor(
    data,
    poison_rate=0.05,
    trigger_value=0.1  # When sepal_width=0.1 â†’ always "virginica"
)
```

**Characteristics**:
- **Detection**: Extremely hard (normal performance on clean data)
- **Impact**: Complete control when trigger present
- **Mitigation**: 10-30% recovery rate

**Example Effect**:
```
Training: [5.1, 0.1, 1.4, 0.2] â†’ "virginica"  â† Trigger
          [6.3, 0.1, 4.9, 1.5] â†’ "virginica"  â† Trigger

Attack: ANY sample with sepal_width=0.1 â†’ misclassified as "virginica"
```

**Real-world Analogy**: Supply chain attacks, hidden functionality

---

## ğŸ” Validation Methods

### 1. Statistical Outlier Detection (Z-score)

**Method**: Flag samples > 3 standard deviations from mean

```python
validator = DataValidator()
report = validator.comprehensive_validation(data)

outliers = report['statistical_outliers']
print(f"Total outliers: {outliers['total_outliers']}")
print(f"Percentage: {outliers['outlier_percentage']:.2f}%")
```

**Formula**: `Z = (value - mean) / std`

**Threshold**: `|Z| > 3` indicates outlier

**Effectiveness**:
- âœ… Random noise: Excellent
- âš ï¸ Label flip: Poor (no feature outliers)
- âš ï¸ Targeted: Moderate
- âŒ Backdoor: Poor

---

### 2. Isolation Forest (ML-based)

**Method**: Anomalies are easier to isolate in random trees

```python
validator = DataValidator()
report = validator.comprehensive_validation(data)

anomalies = report['isolation_forest_anomalies']
print(f"Total anomalies: {anomalies['total_anomalies']}")
```

**How it works**: 
- Builds random decision trees
- Anomalies require fewer splits to isolate
- Lower path length â†’ more anomalous

**Effectiveness**:
- âœ… Random noise: Excellent
- âœ… Label flip: Good
- âš ï¸ Targeted: Moderate
- âŒ Backdoor: Poor

---

### 3. Label Distribution Analysis (Chi-square)

**Method**: Test if classes are balanced using chi-square statistic

```python
validator = DataValidator()
report = validator.comprehensive_validation(data)

distribution = report['label_distribution']
print(f"Is balanced: {distribution['is_balanced']}")
print(f"Chi-square: {distribution['chi2_statistic']:.2f}")
```

**Threshold**: Ï‡Â² < 10 indicates balanced

**Effectiveness**:
- âŒ Random noise: Not applicable
- âœ… Label flip: Excellent
- âš ï¸ Targeted: Moderate
- âŒ Backdoor: Poor

---

### 4. Feature Range Validation

**Method**: Check if values are within biologically plausible ranges

```python
validator = DataValidator()
report = validator.comprehensive_validation(data)

violations = report['feature_range_violations']
print(f"Has violations: {violations['has_violations']}")
print(f"Total violations: {violations['total_violations']}")
```

**Expected Ranges** (IRIS dataset):
```python
{
    'sepal_length': (4.0, 8.0),
    'sepal_width': (2.0, 4.5),
    'petal_length': (1.0, 7.0),
    'petal_width': (0.1, 2.5)
}
```

**Effectiveness**:
- âœ… Random noise: Excellent
- âŒ Label flip: Not applicable
- âš ï¸ Targeted: Moderate
- âŒ Backdoor: Poor

---

### 5. Comprehensive Health Score

**Method**: Combines all checks into 0-100 score

```python
validator = DataValidator()
report = validator.comprehensive_validation(data)

print(f"Health Score: {report['overall_health_score']:.1f}/100")
print(f"Is Clean: {report['is_clean']}")
```

**Score Calculation**:
```
Start: 100 points
- Subtract: Outlier percentage (max -30)
- Subtract: Anomaly percentage (max -30)
- Subtract: Imbalanced labels (-20)
- Subtract: Range violations (max -20)

Result: max(0, final_score)
```

**Interpretation**:
- **90-100**: Excellent (clean data)
- **80-90**: Good (minor issues)
- **70-80**: Fair (investigate)
- **< 70**: Poor (likely poisoned)

---

## ğŸ› ï¸ Mitigation Strategies

### 1. Outlier Removal

**Method**: Remove samples flagged as outliers

```python
mitigator = DataMitigator()
cleaned_data, removed_indices = mitigator.remove_outliers(
    data,
    method='zscore',     # or 'isolation_forest'
    threshold=2.5        # Stricter than detection (3.0)
)

print(f"Removed {len(removed_indices)} samples")
print(f"Cleaned dataset size: {len(cleaned_data)}")
```

**When to use**:
- âœ… Random noise poisoning
- âš ï¸ Sufficient remaining data (>50 samples)
- âŒ Label flip (won't help)

**Trade-off**: May remove valid outliers

---

### 2. Feature Clipping

**Method**: Clip extreme values to valid ranges

```python
mitigator = DataMitigator()
clipped_data = mitigator.clip_features(data)
```

**When to use**:
- âœ… Small datasets (can't afford to lose samples)
- âœ… Known valid ranges
- âš ï¸ Random noise with extreme values

**Trade-off**: Distorts data but preserves samples

---

### 3. Ensemble Filtering

**Method**: Remove samples where multiple models disagree

```python
mitigator = DataMitigator()
filtered_data, removed = mitigator.ensemble_filtering(
    data,
    n_models=5  # Train 5 models via cross-validation
)
```

**How it works**:
1. Train 5 models on different data splits
2. For each sample, check prediction confidence
3. Remove samples with low confidence (bottom 10%)

**When to use**:
- âœ… Label flip poisoning
- âœ… Targeted attacks
- âš ï¸ Requires clean majority (>70%)

**Trade-off**: Computationally expensive

---

### 4. Data Augmentation (Adding Clean Data)

**Method**: Calculate and collect additional clean samples

```python
mitigator = DataMitigator()
requirements = mitigator.calculate_required_clean_samples(
    original_size=100,
    poison_rate=0.10,
    target_accuracy=0.90
)

print(f"Additional clean samples needed: {requirements['additional_clean_samples_needed']}")
print(f"Recommended total size: {requirements['recommended_total_size']}")
```

**Formula**:
```python
additional_needed = original_size Ã— (poison_rate / (1 - poison_rate Ã— 0.6))
```

**When to use**:
- âœ… Any poison type
- âœ… Clean data collection is feasible
- âœ… Poison rate < 30%

**Trade-off**: Requires effort to collect new data

---

### Strategy Selection Guide

| Poison Rate | Recommended Strategy | Expected Recovery |
|-------------|---------------------|-------------------|
| **< 5%** | Outlier removal only | 70-80% |
| **5-15%** | Outlier removal + 2x clean data | 80-90% |
| **15-30%** | Multi-method + 5x clean data | 60-80% |
| **> 30%** | Discard & collect fresh dataset | 100% (fresh start) |

---

## ğŸ“Š Understanding Results

### Expected Results by Attack Type
#### Baseline (Clean Data)
```
Accuracy: 96.00%
Precision: 96.05%
Recall: 96.00%
F1 Score: 96.01%
Health Score: 98.5/100
```

#### Random Noise Attacks

| Poison Rate | Accuracy | Health Score | Outliers Detected | Mitigation Recovery |
|-------------|----------|--------------|-------------------|---------------------|
| 5% | 93.2% (-2.8%) | 87.3/100 | 8 samples (~5%) | +3.0% â†’ 96.2% |
| 10% | 89.5% (-6.5%) | 76.8/100 | 16 samples (~11%) | +3.7% â†’ 93.2% |
| 50% | 68.3% (-27.7%) | 34.2/100 | 78 samples (~52%) | +8.5% â†’ 76.8% |

**Observation**: Z-score detection works well, mitigation partially effective

---

#### Label Flip Attacks

| Poison Rate | Accuracy | Health Score | Detected Anomalies | Mitigation Recovery |
|-------------|----------|--------------|---------------------|---------------------|
| 5% | 91.8% (-4.2%) | 92.1/100 | 7 samples | +2.5% â†’ 94.3% |
| 10% | 85.7% (-10.3%) | 81.5/100 | 15 samples | +5.2% â†’ 90.9% |
| 50% | 54.2% (-41.8%) | 28.9/100 | 76 samples | +12.3% â†’ 66.5% |

**Observation**: More damaging, harder to detect (features unchanged)

---

#### Targeted Attacks (Setosa â†’ Virginica)

| Poison Rate | Overall Acc | Setosa Recall | Virginica Prec | Asymmetric Impact |
|-------------|-------------|---------------|----------------|-------------------|
| 5% | 92.5% (-3.5%) | 84.0% (-12%) | 88.2% (-7.8%) | âš ï¸ Target class severely affected |
| 10% | 87.3% (-8.7%) | 72.0% (-24%) | 81.5% (-14.5%) | âš ï¸ Cascading to other classes |
| 50% | 65.8% (-30.2%) | 28.0% (-68%) | 58.3% (-37.7%) | âŒ Complete failure for target |

**Observation**: Overall metrics mask asymmetric damage - always check per-class!

---

### Interpreting Your Output

From your run:
```
INFO:__main__:BASELINE EXPERIMENT: Clean Data
INFO:__main__:  Accuracy:  0.9677
INFO:data_poisoning:Data health score: 90.1/100

INFO:__main__:EXPERIMENT: RANDOM_NOISE @ 10.0%
INFO:__main__:  Accuracy:  0.9032  â† Dropped 6.45%
INFO:data_poisoning:Data health score: 66.1/100  â† Significant degradation
INFO:data_poisoning:Removed 6 outliers using zscore
INFO:__main__:    True Positives: 6  â† Found all 6!
INFO:__main__:    Precision: 1.000  â† Perfect precision
INFO:__main__:    Recall: 0.600  â† But only 60% of total poison
INFO:__main__:After Mitigation:
INFO:__main__:  Accuracy:  0.8966  â† Small recovery (0.66%)
```

**Analysis**:
1. âœ… Detection worked perfectly (Precision = 1.0)
2. âš ï¸ Missed 40% of poison (Recall = 0.6)
3. âš ï¸ Mitigation barely helped (accuracy still low)
4. ğŸ’¡ **Key insight**: Model already learned from poison during training!

**Recommendation**: Prevention > Detection > Mitigation

---

## ğŸ’° Data Quantity vs Quality

### The Core Trade-off

**Question**: Is it better to clean existing data or collect more?

**Answer**: Depends on poison rate!

### Mathematical Relationship

```python
additional_clean_samples = original_size Ã— (poison_rate / (1 - poison_rate Ã— 0.6))
```

**Intuition**: Dilute poison by adding clean data

### Practical Examples

#### Example 1: Low Poison (10%)
```
Dataset: 100 samples, 10% poisoned
Current accuracy: 90%

Option A: Clean existing data
  - Cost: 2 hours manual review
  - Result: 92-93% accuracy
  - Total samples: 90 (removed 10)

Option B: Collect more data
  - Cost: 4 hours collection
  - Need: +11 clean samples
  - Result: 93-94% accuracy
  - Total samples: 111

Verdict: Clean existing âœ… (cheaper, similar result)
```

#### Example 2: High Poison (30%)
```
Dataset: 100 samples, 30% poisoned
Current accuracy: 72%

Option A: Extensive cleaning
  - Cost: 20 hours review + mitigation
  - Result: 80-82% accuracy (still degraded)
  - Total samples: 70-80

Option B: Fresh collection
  - Cost: 30 hours collection
  - Need: 100 new clean samples
  - Result: 96% accuracy
  - Total samples: 100

Verdict: Fresh collection âœ… (better ROI)
```

### Exponential Growth Chart

```
Additional Clean Samples Needed vs Poison Rate

Samples
   150â”‚                                        â—
      â”‚                                   â—
   100â”‚                              â—
      â”‚                         â—
    50â”‚                    â—
      â”‚               â—
     0â”‚          â—
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        0%    10%    20%    30%    40%    50%
                    Poison Rate
```

**Key Insight**: Beyond 30%, exponential growth makes collection more efficient than mitigation

---

### Cost-Benefit Matrix

| Poison Rate | Clean Data Cost | Additional Data Cost | Recommendation |
|-------------|-----------------|----------------------|----------------|
| < 5% | $100 | $400 | Clean existing âœ… |
| 5-10% | $300 | $500 | Clean existing âœ… |
| 10-20% | $800 | $1000 | Borderline âš ï¸ |
| 20-30% | $1500 | $1500 | Either option âš ï¸ |
| > 30% | $2500 | $1800 | Collect fresh âœ… |

*Assumes: Cleaning = $50/hour, Collection = $100/hour*

---

## ğŸ”¬ MLflow Integration

### Experiment Structure

```
MLflow Experiment: iris-poisoning-experiments
â”‚
â”œâ”€â”€ Run: baseline_clean_data
â”‚   â”œâ”€â”€ Parameters:
â”‚   â”‚   â”œâ”€â”€ poison_type: none
â”‚   â”‚   â”œâ”€â”€ poison_rate: 0.0
â”‚   â”‚   â””â”€â”€ data_size: 101
â”‚   â”œâ”€â”€ Metrics:
â”‚   â”‚   â”œâ”€â”€ accuracy: 0.9677
â”‚   â”‚   â”œâ”€â”€ data_health_score: 90.1
â”‚   â”‚   â””â”€â”€ f1_score: 0.9675
â”‚   â””â”€â”€ Artifacts:
â”‚       â””â”€â”€ validation_report.json
â”‚
â”œâ”€â”€ Run: random_noise_5pct
â”‚   â”œâ”€â”€ Parameters:
â”‚   â”‚   â”œâ”€â”€ poison_type: random_noise
â”‚   â”‚   â”œâ”€â”€ poison_rate: 0.05
â”‚   â”‚   â””â”€â”€ n_poisoned_samples: 5
â”‚   â”œâ”€â”€ Metrics:
â”‚   â”‚   â”œâ”€â”€ accuracy: 0.9355
â”‚   â”‚   â”œâ”€â”€ mitigated_accuracy: 0.9000
â”‚   â”‚   â”œâ”€â”€ data_health_score: 75.2
â”‚   â”‚   â”œâ”€â”€ detected_outliers: 3
â”‚   â”‚   â”œâ”€â”€ mitigation_precision: 1.0
â”‚   â”‚   â””â”€â”€ mitigation_recall: 0.6
â”‚   â””â”€â”€ Artifacts:
â”‚       â”œâ”€â”€ validation_report.json
â”‚       â””â”€â”€ data_requirements.json
â”‚
â”œâ”€â”€ Run: random_noise_10pct
â”œâ”€â”€ Run: random_noise_50pct
â”œâ”€â”€ Run: label_flip_5pct
â”œâ”€â”€ Run: label_flip_10pct
â”œâ”€â”€ Run: label_flip_50pct
â”œâ”€â”€ Run: targeted_5pct
â”œâ”€â”€ Run: targeted_10pct
â”œâ”€â”€ Run: targeted_50pct
â”‚
â””â”€â”€ Run: comprehensive_analysis
    â”œâ”€â”€ Artifacts:
    â”‚   â”œâ”€â”€ accuracy_comparison.png
    â”‚   â”œâ”€â”€ data_requirements.png
    â”‚   â””â”€â”€ poisoning_analysis_report.md
    â””â”€â”€ Tags:
        â””â”€â”€ final_summary
```

### Viewing Results

```bash
# Start MLflow UI
mlflow ui --port 5001

# Open browser
open http://localhost:5001
```

### Comparing Runs

1. **Select experiments** to compare
2. **View metrics chart**: accuracy, health_score, etc.
3. **Compare parameters**: poison_type, poison_rate
4. **Download artifacts**: reports, charts

### Querying via API

```python
import mlflow

# Set tracking URI
mlflow.set_tracking_uri("http://localhost:5001")

# Get experiment
experiment = mlflow.get_experiment_by_name("iris-poisoning-experiments")

# Get all runs
runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])

# Filter by poison type
random_noise_runs = runs[runs['params.poison_type'] == 'random_noise']

# Get best run
best_run = runs.loc[runs['metrics.accuracy'].idxmax()]
print(f"Best accuracy: {best_run['metrics.accuracy']}")
```

---

## ğŸ”„ CI/CD Integration

### GitHub Actions Workflow

The framework includes automated validation via GitHub Actions.

#### Workflow Triggers

1. **Manual**: Via workflow dispatch
2. **Scheduled**: Weekly on Sundays
3. **On-demand**: Custom poison rates

#### Workflow Steps

```yaml
1. Setup Environment
   â”œâ”€â”€ Checkout code
   â”œâ”€â”€ Install dependencies
   â””â”€â”€ Setup DVC & pull data

2. Validate Baseline
   â””â”€â”€ Check clean data health score

3. Run Poisoning Experiments
   â”œâ”€â”€ Apply attacks (5%, 10%, 50%)
   â”œâ”€â”€ Validate poisoned data
   â”œâ”€â”€ Train & evaluate models
   â””â”€â”€ Log to MLflow

4. Generate Reports
   â”œâ”€â”€ Create comparison charts
   â”œâ”€â”€ Generate markdown report
   â””â”€â”€ Upload artifacts

5. Post Results
   â”œâ”€â”€ Comment on PR
   â””â”€â”€ Fail if health < 80
```

#### Triggering the Workflow

```bash
# Via GitHub UI
GitHub â†’ Actions â†’ "Data Poisoning Validation" â†’ Run workflow

# Set custom parameters
Inputs:
  poison_rates: "0.05,0.15,0.25"
```

#### Viewing Results

- **GitHub Actions tab**: Real-time logs
- **Artifacts section**: Download reports & charts
- **PR Comments**: CML report with findings

---

## ğŸ“š API Reference

### DataPoisoner Class

```python
from data_poisoning import DataPoisoner

poisoner = DataPoisoner(random_state=42)
```

#### Methods

**`poison_random_noise(data, poison_rate, noise_level=3.0)`**
- **Parameters**:
  - `data` (DataFrame): Clean dataset
  - `poison_rate` (float): Percentage to poison (0.0-1.0)
  - `noise_level` (float): Standard deviations of noise
- **Returns**: `(poisoned_data, poison_indices)`

**`poison_label_flip(data, poison_rate)`**
- **Parameters**:
  - `data` (DataFrame): Clean dataset
  - `poison_rate` (float): Percentage to poison
- **Returns**: `(poisoned_data, poison_indices)`

**`poison_targeted_attack(data, poison_rate, target_class, misclassify_as)`**
- **Parameters**:
  - `data` (DataFrame): Clean dataset
  - `poison_rate` (float): Percentage of target class to poison
  - `target_class` (str): Class to attack
  - `misclassify_as` (str): Target misclassification
- **Returns**: `(poisoned_data, poison_indices)`

**`poison_backdoor(data, poison_rate, trigger_value=0.1)`**
- **Parameters**:
  - `data` (DataFrame): Clean dataset
  - `poison_rate` (float): Percentage to inject trigger
  - `trigger_value` (float): Backdoor trigger pattern
- **Returns**: `(poisoned_data, poison_indices)`

---

### DataValidator Class

```python
from data_poisoning import DataValidator

validator = DataValidator()
```

#### Methods

**`comprehensive_validation(data)`**
- **Parameters**: `data` (DataFrame): Dataset to validate
- **Returns**: Dictionary with:
  - `data_hash`: SHA256 hash
  - `dataset_size`: Number of samples
  - `statistical_outliers`: Z-score based detection
  - `isolation_forest_anomalies`: ML-based detection
  - `label_distribution`: Chi-square test results
  - `feature_range_violations`: Boundary checks
  - `overall_health_score`: 0-100 quality metric
  - `is_clean`: Boolean (score > 80)

**`detect_outliers_statistical(data)`**
- **Parameters**: `data` (DataFrame): Dataset to check
- **Returns**: Dictionary with outlier counts and indices

**`detect_outliers_isolation_forest(data)`**
- **Parameters**: `data` (DataFrame): Dataset to check
- **Returns**: Dictionary with anomaly scores and indices

**`check_label_distribution(data)`**
- **Parameters**: `data` (DataFrame): Dataset to check
- **Returns**: Dictionary with distribution and balance metrics

**`check_feature_ranges(data)`**
- **Parameters**: `data` (DataFrame): Dataset to check
- **Returns**: Dictionary with range violations

**`calculate_data_hash(data)`**
- **Parameters**: `data` (DataFrame): Dataset to hash
- **Returns**: SHA256 hash string

---

### DataMitigator Class

```python
from data_poisoning import DataMitigator

mitigator = DataMitigator()
```

#### Methods

**`remove_outliers(data, method='zscore', threshold=3.0)`**
- **Parameters**:
  - `data` (DataFrame): Dataset to clean
  - `method` (str): 'zscore' or 'isolation_forest'
  - `threshold` (float): Z-score threshold (default: 3.0)
- **Returns**: `(cleaned_data, removed_indices)`

**`clip_features(data)`**
- **Parameters**: `data` (DataFrame): Dataset to clip
- **Returns**: Clipped DataFrame

**`ensemble_filtering(data, n_models=5)`**
- **Parameters**:
  - `data` (DataFrame): Dataset to filter
  - `n_models` (int): Number of models in ensemble
- **Returns**: `(filtered_data, removed_indices)`

**`calculate_required_clean_samples(original_size, poison_rate, target_accuracy=0.90)`**
- **Parameters**:
  - `original_size` (int): Original dataset size
  - `poison_rate` (float): Percentage poisoned (0.0-1.0)
  - `target_accuracy` (float): Target accuracy to achieve
- **Returns**: Dictionary with:
  - `original_size`: Original dataset size
  - `poisoned_samples`: Number of poison samples
  - `clean_samples`: Number of clean samples
  - `poison_rate`: Poison percentage
  - `estimated_accuracy_loss`: Expected accuracy drop
  - `additional_clean_samples_needed`: Clean samples to add
  - `recommended_total_size`: New total dataset size
  - `new_poison_rate`: Diluted poison rate
  - `strategy`: Recommended mitigation strategy

---

### PoisoningExperiment Class

```python
from run_poisoning_experiments import PoisoningExperiment

experiment = PoisoningExperiment(
    data_path='iris-dvc-pipeline/v1_data.csv',
    mlflow_tracking_uri='http://localhost:5001',
    experiment_name='iris-poisoning-experiments'
)
```

#### Methods

**`run_baseline_experiment()`**
- **Returns**: Baseline metrics dictionary
- **MLflow**: Logs 1 run with clean data results

**`run_poisoning_experiment(poison_type, poison_rates)`**
- **Parameters**:
  - `poison_type` (str): 'random_noise', 'label_flip', 'targeted', 'backdoor'
  - `poison_rates` (list): List of rates to test (e.g., [0.05, 0.10, 0.50])
- **Returns**: Results dictionary for all rates
- **MLflow**: Logs N runs (one per rate)

**`generate_comparison_report(all_results)`**
- **Parameters**: `all_results` (dict): Combined results from all experiments
- **Generates**:
  - `accuracy_comparison.png`: Bar chart
  - `data_requirements.png`: Line plot
  - `poisoning_analysis_report.md`: Comprehensive report
- **MLflow**: Logs 1 run with all artifacts

---

## ğŸ¯ Advanced Usage

### Custom Attack Development

Add your own attack type:

```python
# In data_poisoning.py, add to DataPoisoner class

def poison_custom_attack(self, data, poison_rate, **kwargs):
    """
    Your custom poisoning logic.
    
    Args:
        data: Original DataFrame
        poison_rate: Percentage to poison
        **kwargs: Additional parameters
    
    Returns:
        Tuple of (poisoned_data, poison_indices)
    """
    poisoned_data = data.copy()
    n_poison = int(len(data) * poison_rate)
    poison_indices = np.random.choice(len(data), n_poison, replace=False)
    
    # Your custom poisoning logic here
    for idx in poison_indices:
        # Example: Flip specific features
        poisoned_data.loc[idx, 'petal_length'] = 0.0
    
    return poisoned_data, poison_indices.tolist()
```

Then use it in experiments:

```python
# In run_poisoning_experiments.py

poison_types = ['random_noise', 'label_flip', 'targeted', 'custom']

# Add handling in run_poisoning_experiment()
elif poison_type == "custom":
    poisoned_data, poison_indices = self.poisoner.poison_custom_attack(
        self.clean_data, poison_rate
    )
```

---

### Custom Mitigation Strategy

Implement your own mitigation:

```python
# In data_poisoning.py, add to DataMitigator class

def custom_mitigation(self, data, **kwargs):
    """
    Your custom mitigation logic.
    
    Args:
        data: Poisoned DataFrame
        **kwargs: Additional parameters
    
    Returns:
        Tuple of (cleaned_data, removed_indices)
    """
    cleaned_data = data.copy()
    removed_indices = []
    
    # Your logic here
    # Example: Remove samples with extreme feature combinations
    condition = (data['sepal_length'] > 7.0) & (data['petal_width'] < 0.5)
    suspicious = data[condition].index.tolist()
    
    cleaned_data = data.drop(suspicious).reset_index(drop=True)
    removed_indices = suspicious
    
    return cleaned_data, removed_indices
```

---

### Batch Processing

Process multiple datasets:

```python
import glob
from run_poisoning_experiments import PoisoningExperiment

datasets = glob.glob('data/*.csv')

for dataset_path in datasets:
    print(f"\nProcessing: {dataset_path}")
    
    experiment = PoisoningExperiment(
        data_path=dataset_path,
        mlflow_tracking_uri='http://localhost:5001',
        experiment_name=f'poisoning-{dataset_path.split("/")[-1]}'
    )
    
    # Run baseline
    baseline = experiment.run_baseline_experiment()
    
    # Run attacks
    results = {}
    for poison_type in ['random_noise', 'label_flip']:
        results[poison_type] = experiment.run_poisoning_experiment(
            poison_type, [0.05, 0.10]
        )
    
    # Generate report
    results['baseline'] = baseline
    experiment.generate_comparison_report(results)
```

---

### Programmatic Analysis

Analyze results without running experiments:

```python
import mlflow
import pandas as pd

# Connect to MLflow
mlflow.set_tracking_uri("http://localhost:5001")

# Get experiment
experiment = mlflow.get_experiment_by_name("iris-poisoning-experiments")

# Query all runs
runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])

# Analysis
print("=== Summary Statistics ===")
print(f"Total runs: {len(runs_df)}")
print(f"\nAverage accuracy by poison type:")
print(runs_df.groupby('params.poison_type')['metrics.accuracy'].mean())

print(f"\nAccuracy degradation by poison rate:")
baseline_acc = runs_df[runs_df['params.poison_type'] == 'none']['metrics.accuracy'].values[0]
print(runs_df.groupby('params.poison_rate').apply(
    lambda x: baseline_acc - x['metrics.accuracy'].mean()
))

# Visualization
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(10, 6))
for poison_type in ['random_noise', 'label_flip', 'targeted']:
    subset = runs_df[runs_df['params.poison_type'] == poison_type]
    ax.plot(
        subset['params.poison_rate'].astype(float) * 100,
        subset['metrics.accuracy'] * 100,
        marker='o',
        label=poison_type.replace('_', ' ').title()
    )

ax.axhline(y=baseline_acc * 100, linestyle='--', color='gray', label='Baseline')
ax.set_xlabel('Poison Rate (%)')
ax.set_ylabel('Accuracy (%)')
ax.set_title('Accuracy Degradation by Attack Type')
ax.legend()
ax.grid(True, alpha=0.3)
plt.savefig('custom_analysis.png', dpi=300)
```

---

### Integration with Existing Pipeline

Add validation to your training script:

```python
# In main.py or training script

from data_poisoning import DataValidator

# After loading data
data = pd.read_csv('iris-dvc-pipeline/v1_data.csv')

# Validate before training
validator = DataValidator()
report = validator.comprehensive_validation(data)

if not report['is_clean']:
    logger.warning(f"âš ï¸ Data quality issues detected!")
    logger.warning(f"Health Score: {report['overall_health_score']:.1f}/100")
    logger.warning(f"Outliers: {report['statistical_outliers']['total_outliers']}")
    
    # Decide whether to proceed
    if report['overall_health_score'] < 70:
        raise ValueError("Data quality too poor for training. Manual review required.")
    else:
        logger.info("Proceeding with caution...")

# Continue with training
model.fit(X_train, y_train)
```

---

## ğŸ› Troubleshooting

### Issue: MLflow UI Shows No Experiments

**Symptoms**:
```
MLflow UI is empty
No experiments visible
```

**Cause**: Experiments logged locally instead of to server

**Solution**:
```bash
# Option 1: Specify tracking URI when running
python run_poisoning_experiments.py \
  --mlflow-tracking-uri http://localhost:5001 \
  --data-path iris-dvc-pipeline/v1_data.csv

# Option 2: View local experiments
mlflow ui --backend-store-uri file:///$(pwd)/mlruns --port 5002
open http://localhost:5002
```

---

### Issue: Import Errors

**Symptoms**:
```python
ModuleNotFoundError: No module named 'scipy'
ModuleNotFoundError: No module named 'data_poisoning'
```

**Solution**:
```bash
# Install missing dependencies
pip install scipy seaborn matplotlib

# Ensure you're in correct directory
cd /path/to/your/repo
python -c "import data_poisoning; print('âœ… OK')"
```

---

### Issue: Low Mitigation Recovery

**Symptoms**:
```
Mitigation barely improves accuracy
Recovery rate < 10%
```

**Cause**: Model already learned from poison during training

**Explanation**:
- Removing outliers AFTER training doesn't help much
- Model internalized wrong patterns
- Need to train fresh model on cleaned data

**Solution**:
```python
# Correct approach: Clean BEFORE training

# 1. Load data
data = pd.read_csv('data.csv')

# 2. Clean data
mitigator = DataMitigator()
cleaned_data, removed = mitigator.remove_outliers(data)

# 3. Train on cleaned data (not original!)
X_train, X_test, y_train, y_test = train_test_split(cleaned_data, ...)
model.fit(X_train, y_train)
```

---

### Issue: "Too few samples after mitigation"

**Symptoms**:
```
WARNING:__main__:Too few samples after mitigation!
Accuracy: 0.0000
```

**Cause**: Removed too many samples, not enough left for training

**Solution**:
```python
# Use less aggressive threshold
cleaned_data, removed = mitigator.remove_outliers(
    data,
    method='zscore',
    threshold=3.5  # More lenient (was 2.5)
)

# Or use clipping instead
clipped_data = mitigator.clip_features(data)
```

---

### Issue: Health Score Doesn't Match Expectations

**Symptoms**:
```
Label flip @ 50% poison
Health Score: 90.1/100  â† Should be lower!
```

**Explanation**: Label flip keeps features unchanged
- Z-score sees no outliers
- Isolation Forest sees no anomalies
- Only label distribution affected

**Solution**: Check multiple indicators
```python
report = validator.comprehensive_validation(data)

print(f"Health Score: {report['overall_health_score']}")
print(f"Outliers: {report['statistical_outliers']['total_outliers']}")
print(f"Label balanced: {report['label_distribution']['is_balanced']}")

# For label flip, check distribution specifically
if not report['label_distribution']['is_balanced']:
    print("âš ï¸ Label distribution anomaly detected!")
```

---

### Issue: Experiments Taking Too Long

**Symptoms**:
```
Run time > 30 minutes
Hangs on ensemble filtering
```

**Solution**:
```bash
# Run fewer poison rates
python run_poisoning_experiments.py \
  --poison-rates 0.10  # Just one rate instead of three

# Skip ensemble filtering (expensive)
# In _run_mitigation(), comment out:
# mitigated_metrics = self._run_mitigation(...)

# Use smaller n_models
mitigator.ensemble_filtering(data, n_models=3)  # Instead of 5
```

---

## ğŸ“– References & Further Reading

### Academic Papers

1. **Biggio, B., et al.** "Poisoning Attacks against Support Vector Machines." *ICML 2012*
   - Seminal work on data poisoning

2. **Steinhardt, J., et al.** "Certified Defenses for Data Poisoning Attacks." *NeurIPS 2017*
   - Theoretical guarantees against poisoning

3. **Koh, P. W., & Liang, P.** "Understanding Black-box Predictions via Influence Functions." *ICML 2017*
   - Measuring training data influence

4. **Gu, T., et al.** "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain." *2017*
   - Backdoor attacks in neural networks

### Tutorials & Guides

- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [Scikit-learn Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html)
- [Adversarial ML - OWASP](https://owasp.org/www-project-machine-learning-security-top-10/)

### Related Projects

- [CleverHans](https://github.com/cleverhans-lab/cleverhans) - Adversarial examples library
- [ART (Adversarial Robustness Toolbox)](https://github.com/Trusted-AI/adversarial-robustness-toolbox)
- [TextAttack](https://github.com/QData/TextAttack) - Adversarial attacks for NLP

---

## ğŸ¤ Contributing

### How to Contribute

1. **Fork the repository**
2. **Create feature branch**: `git checkout -b feature/new-attack-type`
3. **Make changes**: Add new attack, mitigation, or analysis
4. **Test thoroughly**: Run experiments and verify results
5. **Submit PR**: Include description and example results

### Contribution Ideas

- âœ¨ New attack types (e.g., gradient-based, adaptive)
- âœ¨ Additional mitigation strategies (e.g., robust training)
- âœ¨ Support for other datasets (not just IRIS)
- âœ¨ Performance optimizations
- âœ¨ Additional visualizations
- âœ¨ Documentation improvements

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **IRIS Dataset**: UCI Machine Learning Repository
- **MLflow**: Databricks & Community
- **Scikit-learn**: For ML utilities
- **Research Community**: For pioneering work on data poisoning

---

## ğŸ“ Support & Contact

### Getting Help

- **Issues**: [GitHub Issues](https://github.com/YOUR_USERNAME/YOUR_REPO/issues)
- **Discussions**: [GitHub Discussions](https://github.com/YOUR_USERNAME/YOUR_REPO/discussions)
- **Documentation**: This README + [DATA_POISONING_ANALYSIS.md](./DATA_POISONING_ANALYSIS.md)

### Quick Links

- **Main README**: [README.md](./README.md) - Overall project documentation
- **Stress Testing**: [STRESS_TESTING.md](./STRESS_TESTING.md) - Auto-scaling guide
- **Quick Reference**: [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) - Command cheat sheet
- **Poisoning Guide**: [POISONING_QUICK_START.md](./POISONING_QUICK_START.md) - Quick start

---

## ğŸ“ Educational Use

This framework is designed for:
- âœ… **Learning**: Understanding data poisoning attacks
- âœ… **Research**: Developing new defenses
- âœ… **Red Team Testing**: Testing ML systems (with permission)
- âœ… **Security Audits**: Validating data pipelines

**Ethical Use Only**: Do not use for malicious purposes or unauthorized attacks

---

## ğŸ“Š Quick Reference

### Essential Commands

```bash
# Run all experiments
python run_poisoning_experiments.py \
  --data-path iris-dvc-pipeline/v1_data.csv \
  --mlflow-tracking-uri http://localhost:5001 \
  --poison-rates 0.05,0.10,0.50

# View results
mlflow ui --port 5001

# Validate data
python -c "from data_poisoning import DataValidator; import pandas as pd; \
validator = DataValidator(); data = pd.read_csv('iris-dvc-pipeline/v1_data.csv'); \
report = validator.comprehensive_validation(data); \
print(f'Health: {report[\"overall_health_score\"]:.1f}/100')"

# Run GitHub workflow
gh workflow run data-poisoning-validation.yml

# Check artifacts
ls -la *.png *.md
```

### Key Files

```
your-repo/
â”œâ”€â”€ data_poisoning.py              # Core classes (Poisoner, Validator, Mitigator)
â”œâ”€â”€ run_poisoning_experiments.py   # Experiment orchestration
â”œâ”€â”€ DATA_POISONING_ANALYSIS.md     # Comprehensive theory & practice
â”œâ”€â”€ DATA_POISONING_README.md       # This file
â””â”€â”€ POISONING_QUICK_START.md       # Quick reference
```

---

## ğŸ¯ Summary

This data poisoning framework provides:

1. **4 Attack Types**: Comprehensive coverage of poisoning scenarios
2. **5 Validation Methods**: Statistical + ML-based detection
3. **4 Mitigation Strategies**: From simple to sophisticated
4. **MLflow Integration**: Full experiment tracking
5. **Automated CI/CD**: GitHub Actions validation
6. **Extensive Documentation**: Theory, practice, and examples

**Key Takeaways**:
- ğŸ›¡ï¸ **Prevention > Detection > Mitigation**
- ğŸ“Š **Data Quality > Data Quantity** (clean beats large)
- ğŸ” **Multi-layer Defense**: Combine multiple strategies
- ğŸ“ˆ **Monitor Continuously**: Production systems need ongoing validation
- ğŸ’° **Cost-Benefit**: Beyond 30% poison, start fresh

**Next Steps**:
1. Run baseline experiment
2. Test different attack types
3. Compare mitigation effectiveness
4. Integrate validation into your pipeline
5. Set up automated monitoring

---

<div align="center">

**â­ If you find this framework useful, please star the repo!**

**Made with â¤ï¸ for ML Security & Robustness**

[Report Bug](https://github.com/YOUR_USERNAME/YOUR_REPO/issues) Â· 
[Request Feature](https://github.com/YOUR_USERNAME/YOUR_REPO/issues) Â· 
[Documentation](./DATA_POISONING_ANALYSIS.md)

</div>
